{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import twitter\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>ISO_TA</th>\n",
       "      <th>Score_TA</th>\n",
       "      <th>Status_TA</th>\n",
       "      <th>Last Update_TA</th>\n",
       "      <th>Source_TA</th>\n",
       "      <th>Score_DOS</th>\n",
       "      <th>Status_DOS</th>\n",
       "      <th>Last Update_DOS</th>\n",
       "      <th>Source_DOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Do Not Travel</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>https://www.travel-advisory.info/afghanistan</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Do Not Travel</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>http://travel.state.gov/content/travel/en/trav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>AL</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Exercise Normal Precautions</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>https://www.travel-advisory.info/albania</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Exercise Normal Precautions</td>\n",
       "      <td>2019-07-10</td>\n",
       "      <td>http://travel.state.gov/content/travel/en/trav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZ</td>\n",
       "      <td>2.8</td>\n",
       "      <td>Exercised Increased Caution</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>https://www.travel-advisory.info/algeria</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Exercise Increased Caution</td>\n",
       "      <td>2019-04-09</td>\n",
       "      <td>http://travel.state.gov/content/travel/en/trav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>AD</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Exercise Normal Precautions</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>https://www.travel-advisory.info/andorra</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Exercise Normal Precautions</td>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>http://travel.state.gov/content/travel/en/trav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angola</td>\n",
       "      <td>AO</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Exercise Normal Precautions</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>https://www.travel-advisory.info/angola</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Exercise Normal Precautions</td>\n",
       "      <td>2019-04-09</td>\n",
       "      <td>http://travel.state.gov/content/travel/en/trav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name ISO_TA  Score_TA                    Status_TA Last Update_TA  \\\n",
       "0  Afghanistan     AF       5.0                Do Not Travel     2020-03-11   \n",
       "1      Albania     AL       1.5  Exercise Normal Precautions     2020-03-11   \n",
       "2      Algeria     DZ       2.8  Exercised Increased Caution     2020-03-11   \n",
       "3      Andorra     AD       1.3  Exercise Normal Precautions     2020-03-11   \n",
       "4       Angola     AO       2.0  Exercise Normal Precautions     2020-03-11   \n",
       "\n",
       "                                      Source_TA  Score_DOS  \\\n",
       "0  https://www.travel-advisory.info/afghanistan        4.0   \n",
       "1      https://www.travel-advisory.info/albania        1.0   \n",
       "2      https://www.travel-advisory.info/algeria        2.0   \n",
       "3      https://www.travel-advisory.info/andorra        1.0   \n",
       "4       https://www.travel-advisory.info/angola        1.0   \n",
       "\n",
       "                    Status_DOS Last Update_DOS  \\\n",
       "0                Do Not Travel      2019-10-22   \n",
       "1  Exercise Normal Precautions      2019-07-10   \n",
       "2   Exercise Increased Caution      2019-04-09   \n",
       "3  Exercise Normal Precautions      2019-08-27   \n",
       "4  Exercise Normal Precautions      2019-04-09   \n",
       "\n",
       "                                          Source_DOS  \n",
       "0  http://travel.state.gov/content/travel/en/trav...  \n",
       "1  http://travel.state.gov/content/travel/en/trav...  \n",
       "2  http://travel.state.gov/content/travel/en/trav...  \n",
       "3  http://travel.state.gov/content/travel/en/trav...  \n",
       "4  http://travel.state.gov/content/travel/en/trav...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common = pd.read_csv('./data/dos_ta_merged.csv', index_col=0)\n",
    "common.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweepy\n",
    "\n",
    "consumer_key = 'fv6RR63biGkVPEbS13GP37EGW'\n",
    "consumer_secret = 'eSx2cTC5DkrTAZCb8Y4NOhjRC3JBPDjq2DZDJAS1YX0RJ2tjiC'\n",
    "access_token = '1235380439457595395-tLGeMVWCBxVNzgBuqCRXw3JiT2IRh7' \n",
    "access_token_secret = '4tvwVHcAFkYPjV4iXQi2TczsIU5qEqbxjYANddPmyGRPn'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "tw = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tw.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = 'from:USATODAY OR from:WSJ OR from:nytimes OR from:washingtonpost OR from:latimes OR from:chicagotribune OR from:guardian OR from:thetimes OR from:Telegraph OR from:FinancialTimes OR from:SCMPNews OR from:timesofindia OR from:japantimes OR from:smh OR from:dwnews OR from:BBCWorld OR from:AP OR from:globeandmail OR from:Reuters'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 554\n"
     ]
    }
   ],
   "source": [
    "tw_corpus = pd.DataFrame(columns=['Tweets','Name'])\n",
    "\n",
    "for name in common.Name.values:\n",
    "    search_query = f'{name} {str(sources)}'\n",
    "    corpus = tw.search(q=search_query, count=100,\n",
    "                       result_type = \"recent\",\n",
    "                       lang = \"en\",\n",
    "                       include_entities=False,\n",
    "                       tweet_mode='extended')\n",
    "    tw_corpus.append([{'Tweets':[tweet.full_text for tweet in corpus]},{'Name': name}], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_corpus['Name'] = tw_corpus['Name'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_corpus = tw_corpus.dropna(axis=0,how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_corpus = tw_corpus.drop(['level_0','index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_corpus = pd.DataFrame(tw_corpus.Tweets.tolist(), index=tw_corpus.Name).stack().reset_index(level=1, drop=True).reset_index(name='Tweets')[['Tweets','Name']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'RT[\\s]+', '', text)\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_corpus['Tweets'] = tw_corpus['Tweets'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The coronavirus doesn’t fit the templates of 9...</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The man with the gun in his hand had jail time...</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pakistan's interior ministry said it will clos...</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan’s Kashmir Fallout\\n\\nHow will like...</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan is preparing to release 1,500 Tali...</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Afghan President Ashraf Ghani signed a decree ...</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Taliban have overtaken Islamic State as th...</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>President Ashraf Ghani of Afghanistan ordered ...</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Afghan government to release Taliban prisoners...</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hundreds of British troops will be withdrawn f...</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets         Name\n",
       "0  The coronavirus doesn’t fit the templates of 9...  Afghanistan\n",
       "1  The man with the gun in his hand had jail time...  Afghanistan\n",
       "2  Pakistan's interior ministry said it will clos...  Afghanistan\n",
       "3  Afghanistan’s Kashmir Fallout\\n\\nHow will like...  Afghanistan\n",
       "4  Afghanistan is preparing to release 1,500 Tali...  Afghanistan\n",
       "5  Afghan President Ashraf Ghani signed a decree ...  Afghanistan\n",
       "6  The Taliban have overtaken Islamic State as th...  Afghanistan\n",
       "7  President Ashraf Ghani of Afghanistan ordered ...  Afghanistan\n",
       "8  Afghan government to release Taliban prisoners...  Afghanistan\n",
       "9  Hundreds of British troops will be withdrawn f...  Afghanistan"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_corpus.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning\n",
    "\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    " \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "#after tweepy preprocessing the colon symbol left remain after      #removing mentions\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "#replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "#remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "#filter using NLTK library append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_tweet = []\n",
    "#looping through conditions\n",
    "    for w in word_tokens:\n",
    "#check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "    return ' '.join(filtered_tweet)\n",
    "    #print(word_tokens)\n",
    "    #print(filtered_sentence)return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_corpus['Tweets'] = tw_corpus['Tweets'].apply(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_corpus = tw_corpus.drop('Tweet', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_corpus.to_csv('/Users/alejandroarrya/Desktop/Ironhack/DAPTMX/final_project/TASAM_final_project/data/tw_corpus.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(tweet):\n",
    "    \n",
    "    #Generating the list of words in the tweet (hastags and other punctuations removed)\n",
    "    def form_sentence(tweet):\n",
    "        tweet_blob = TextBlob(tweet)\n",
    "        return ' '.join(tweet_blob.words)\n",
    "    new_tweet = form_sentence(tweet)\n",
    "    \n",
    "    #Removing stopwords and words with unusual symbols\n",
    "    def no_user_alpha(tweet):\n",
    "        tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "        clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "        clean_s = ' '.join(clean_tokens)\n",
    "        clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]\n",
    "        return clean_mess\n",
    "    no_punc_tweet = no_user_alpha(new_tweet)\n",
    "    \n",
    "    #Normalizing the words in tweets \n",
    "    def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return normalized_tweet\n",
    "    \n",
    "    \n",
    "    return normalization(no_punc_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
